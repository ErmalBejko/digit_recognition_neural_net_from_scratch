{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "0       0  ...         0         0         0         0         0         0   \n",
      "1       0  ...         0         0         0         0         0         0   \n",
      "2       0  ...         0         0         0         0         0         0   \n",
      "3       0  ...         0         0         0         0         0         0   \n",
      "4       0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0       0       0       0       0       0       0       0       0       0   \n",
      "1       0       0       0       0       0       0       0       0       0   \n",
      "2       0       0       0       0       0       0       0       0       0   \n",
      "3       0       0       0       0       0       0       0       0       0   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "0       0  ...         0         0         0         0         0         0   \n",
      "1       0  ...         0         0         0         0         0         0   \n",
      "2       0  ...         0         0         0         0         0         0   \n",
      "3       0  ...         0         0         0         0         0         0   \n",
      "4       0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 784 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import data and inspect it\n",
    "data = pd.read_csv(\"nn_train.csv\")\n",
    "data_test = pd.read_csv(\"nn_test.csv\")\n",
    "print(data.head())\n",
    "print(data_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000 785\n"
     ]
    }
   ],
   "source": [
    "# Transform the data into np array, print the shape\n",
    "data = np.array(data)\n",
    "data_test = np.array(data_test)\n",
    "m, n = data.shape\n",
    "\n",
    "# Randomly shuffle for each full run\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# See that the dimensions are 42000 rows(data inputs/pictures in this case) and 785 columns (784 features/pixels + 1 label)\n",
    "print(m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 784\n"
     ]
    }
   ],
   "source": [
    "# Split into data_dev to use for test, transpose it so that each column is an input\n",
    "data_dev = data[0:1000].T\n",
    "data_test_dev = data_test.T\n",
    "\n",
    "# Set the label for the test data as the first row of the transposed data\n",
    "Y_dev = data_dev[0]\n",
    "\n",
    "# Define the features set of the test data and rescale between 0 and 1 by dividing with 255\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "X_dev_test = data_test_dev / 255.\n",
    "print(len(X_dev), len(X_dev_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(41000,)\n"
     ]
    }
   ],
   "source": [
    "# Define the training data as the 41000 next rows, then transpose\n",
    "data_train = data[1000:m].T\n",
    "\n",
    "# Define the label train data as the first row of the transposed dataset\n",
    "Y_train = data_train[0]\n",
    "\n",
    "# Define the features set and their values for the train dataset and rescale between 0 and 1 by dividing by 255\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape\n",
    "print(X_train)\n",
    "print(Y_train.shape) # Label for the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define random weights and biases\n",
    "def init_params():\n",
    "    \"\"\"\n",
    "    Initialize the weights and biases from a uniform distribution of -1 to 1\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "        \n",
    "    Returns:\n",
    "        W1: np.array of dimensions ([neurons of next layer] x [number of features]) --> matrix\n",
    "        b1: np.array of dimensions ([nueurons of next layer] x 1) --> vector\n",
    "        W2: np.array of dimensions ([neurons of next layer] x [neurons of previous layer]) --> matrix\n",
    "        b1: np.array of dimensions ([nueurons of next layer] x 1) --> vector        \n",
    "    \"\"\"\n",
    "   \n",
    "    W1 = np.random.rand(196, 784) - 0.5\n",
    "    b1 = np.random.rand(196, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 196) - 0.5 \n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Define ReLU activation function    \n",
    "def ReLU(Z):\n",
    "    \"\"\"\n",
    "    Calculate ReLU\n",
    "    \n",
    "    Args:\n",
    "        Z (np.array): The only parameter\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Each positive element stays the same while the negative ones become 0\n",
    "    \"\"\"\n",
    "    \n",
    "    # ReLU([1 3 -3 0 -2 4]) \n",
    "    # result = [1 3 0 0 0 4]\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"Define the sigmoid func\n",
    "\n",
    "    Args:\n",
    "        Z (np.array): Array/layer what we want to activate\n",
    "\n",
    "    Returns:\n",
    "        np.array: Activated layer with sigmoid\n",
    "    \"\"\"    \n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "   \n",
    "# Define softmax, it is a probability density function (pdf)    \n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Calculate softmax\n",
    "    \n",
    "    Args:\n",
    "        Z (np.array): The only parameter\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Same dimension as input, but all scaled so that the sum of elements equals 1 (pdf)\n",
    "    \"\"\"\n",
    "    \n",
    "    # softmax([1, 2, -3, 4, 0])\n",
    "    # result array([4.13393728e-02, 1.12372066e-01, 7.57157024e-04, 8.30323499e-01, 1.52079054e-02])\n",
    "    return np.exp(Z) / sum(np.exp(Z))\n",
    "    \n",
    " \n",
    "# Define forward propagation calculations   \n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    \"\"\"\n",
    "    Calculate the steps for our 2 layers (1 hidden and 1 output)\n",
    "    \n",
    "    Args:\n",
    "        W1 (array): First weights matrix\n",
    "        b1 (array): First bias vector\n",
    "        W2 (array): Second weights matrix\n",
    "        b2 (array): Second bias vector\n",
    "        X (array): Input (data) matrix \n",
    "    \"\"\"\n",
    "    \n",
    "    Z1 = W1.dot(X) + b1\n",
    "    \n",
    "    # Activate second layer (ReLU)\n",
    "    A1 = ReLU(Z1)\n",
    "    \n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    \n",
    "    # Activate output layer (softmax)\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "    \n",
    "    \n",
    "# Define our labels array    \n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "# Define the derivative of ReLU\n",
    "def deriv_ReLU(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def deriv_sigmoid(Z):\n",
    "    return np.exp(-Z) / (1 + np.exp(-Z))**2\n",
    " \n",
    "# Define the calculations for backward propagation    \n",
    "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    \"\"\"\n",
    "    Calculate the increments that we need to adjust in the weights and biases\n",
    "    \n",
    "    Args:\n",
    "        Z1 (array): First calculation W1 * X + b1 output for second(hidden) layer\n",
    "        A1 (array): Activation of the second(hidden) layer\n",
    "        Z2 (array): Second calculation W2 * A1 + b2 output for third(last) layer\n",
    "        A2 (array): Activation of third(last) layer\n",
    "        W1 (array): First weights matrix\n",
    "        W2 (array): Second weights matrix\n",
    "        X (array): Input/features data matrix\n",
    "        Y (array): Labels data matrix\n",
    "        \n",
    "    Returns:\n",
    "        dW1 (array): Increment change for first weights matrix\n",
    "        db1 (array): Increment change for first bias vector\n",
    "        dw2 (array): Increment change for second weights matrix\n",
    "        db2 (array): Increment change for second bias vector\n",
    "    \"\"\"\n",
    "    \n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * deriv_ReLU(Z1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "\n",
    "# Update the weights and biases according to learning rate (alpha)\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1, b1, W2, b2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification of the prediction\n",
    "def get_predictions(A2):\n",
    "    \"\"\"\n",
    "    Gives the prediction\n",
    "    \n",
    "    Args:\n",
    "        A2 (array): This array is a vector and a probability density function for the number of our classes\n",
    "        \n",
    "    Returns:\n",
    "        np.argmax(A2, 0) (int): Outputs the index with the highest probability\n",
    "    \"\"\"\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "\n",
    "# Define how to get the accuracy of the model\n",
    "def get_accuracy(predictions, Y):\n",
    "    \"\"\"\n",
    "    Gives us the accuracy of the model when we have a dataset with the label column (doesn't apply in test datasets without the label column)\n",
    "    \n",
    "    Args:\n",
    "        predictions (array): First variable, it is the prediction of the model\n",
    "        Y (array): Second variable, it is the array with the labels\n",
    "        \n",
    "    Returns:\n",
    "        np.sum(predictions == Y) / Y.size (float): Counts the cases when our prediction was equal to the label, then we divide by total cases Y.size\n",
    "        \n",
    "    \"\"\"\n",
    "    # print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "\n",
    "# Define the gradient descent used to update the weights and biases \n",
    "def grad_descent(X, Y, iterations, alpha):\n",
    "    \"\"\"\n",
    "    Calculate and update the weights and biases\n",
    "    \n",
    "    Args:\n",
    "        X (array): The features set\n",
    "        Y (array): The label(s) set\n",
    "        iterations (int): Number of iterations to adjust the weights and biases, otherwise called epochs\n",
    "        alpha (float): Factor of used to rescale the increments of dW1, db1, dW2, db2, also called learning rate\n",
    "        \n",
    "    Returns:\n",
    "        W1 (array): Updated W1 weights matrix\n",
    "        b1 (array): Updated b1 bias vector\n",
    "        W2 (array): Updated W2 weights matrix\n",
    "        b2 (array): Updated b2 bias vector\n",
    "    \"\"\"\n",
    "    # Generate the weights and biases\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    \n",
    "    # Loop through the iterations/epochs\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        # Calculate and store results from forward propagation\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)      \n",
    "        \n",
    "        # From our results of forward propagation and our weights and biases calculate the increment changes for the weights and biases  \n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        \n",
    "        # Update our weights and biases according to alpha (learning rate)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        \n",
    "        # For every 10 iterations, print the iteration number, the prediction of the model and the accuracy of the model\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print('Accuracy: ', round(get_accuracy(predictions, Y), 4) * 100, '%')\n",
    "    return W1, b1, W2, b2    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Accuracy:  11.3 %\n",
      "Iteration:  10\n",
      "Accuracy:  27.3 %\n",
      "Iteration:  20\n",
      "Accuracy:  36.08 %\n",
      "Iteration:  30\n",
      "Accuracy:  42.51 %\n",
      "Iteration:  40\n",
      "Accuracy:  50.17 %\n",
      "Iteration:  50\n",
      "Accuracy:  58.3 %\n",
      "Iteration:  60\n",
      "Accuracy:  73.35000000000001 %\n",
      "Iteration:  70\n",
      "Accuracy:  78.10000000000001 %\n",
      "Iteration:  80\n",
      "Accuracy:  78.35 %\n",
      "Iteration:  90\n",
      "Accuracy:  81.77 %\n",
      "Iteration:  100\n",
      "Accuracy:  82.72 %\n",
      "Iteration:  110\n",
      "Accuracy:  84.22 %\n",
      "Iteration:  120\n",
      "Accuracy:  85.42999999999999 %\n",
      "Iteration:  130\n",
      "Accuracy:  86.26 %\n",
      "Iteration:  140\n",
      "Accuracy:  87.03999999999999 %\n",
      "Iteration:  150\n",
      "Accuracy:  87.63 %\n",
      "Iteration:  160\n",
      "Accuracy:  88.21 %\n",
      "Iteration:  170\n",
      "Accuracy:  88.73 %\n",
      "Iteration:  180\n",
      "Accuracy:  89.11 %\n",
      "Iteration:  190\n",
      "Accuracy:  89.51 %\n",
      "Iteration:  200\n",
      "Accuracy:  89.92 %\n",
      "Iteration:  210\n",
      "Accuracy:  90.25 %\n",
      "Iteration:  220\n",
      "Accuracy:  90.47 %\n",
      "Iteration:  230\n",
      "Accuracy:  90.72 %\n",
      "Iteration:  240\n",
      "Accuracy:  90.99000000000001 %\n",
      "Iteration:  250\n",
      "Accuracy:  91.2 %\n",
      "Iteration:  260\n",
      "Accuracy:  91.39 %\n",
      "Iteration:  270\n",
      "Accuracy:  91.60000000000001 %\n",
      "Iteration:  280\n",
      "Accuracy:  91.77 %\n",
      "Iteration:  290\n",
      "Accuracy:  91.92 %\n",
      "Iteration:  300\n",
      "Accuracy:  92.05 %\n",
      "Iteration:  310\n",
      "Accuracy:  92.17999999999999 %\n",
      "Iteration:  320\n",
      "Accuracy:  92.32000000000001 %\n",
      "Iteration:  330\n",
      "Accuracy:  92.47 %\n",
      "Iteration:  340\n",
      "Accuracy:  92.63 %\n",
      "Iteration:  350\n",
      "Accuracy:  92.78 %\n",
      "Iteration:  360\n",
      "Accuracy:  92.88 %\n",
      "Iteration:  370\n",
      "Accuracy:  92.99 %\n",
      "Iteration:  380\n",
      "Accuracy:  93.10000000000001 %\n",
      "Iteration:  390\n",
      "Accuracy:  93.19 %\n",
      "Iteration:  400\n",
      "Accuracy:  93.28 %\n",
      "Iteration:  410\n",
      "Accuracy:  93.39 %\n",
      "Iteration:  420\n",
      "Accuracy:  93.46 %\n",
      "Iteration:  430\n",
      "Accuracy:  93.52000000000001 %\n",
      "Iteration:  440\n",
      "Accuracy:  93.62 %\n",
      "Iteration:  450\n",
      "Accuracy:  93.71000000000001 %\n",
      "Iteration:  460\n",
      "Accuracy:  93.81 %\n",
      "Iteration:  470\n",
      "Accuracy:  93.89 %\n",
      "Iteration:  480\n",
      "Accuracy:  93.94 %\n",
      "Iteration:  490\n",
      "Accuracy:  94.01 %\n",
      "Iteration:  500\n",
      "Accuracy:  94.07 %\n",
      "Iteration:  510\n",
      "Accuracy:  94.14 %\n",
      "Iteration:  520\n",
      "Accuracy:  94.19999999999999 %\n",
      "Iteration:  530\n",
      "Accuracy:  94.28999999999999 %\n",
      "Iteration:  540\n",
      "Accuracy:  94.33 %\n",
      "Iteration:  550\n",
      "Accuracy:  94.43 %\n",
      "Iteration:  560\n",
      "Accuracy:  94.5 %\n",
      "Iteration:  570\n",
      "Accuracy:  94.59 %\n",
      "Iteration:  580\n",
      "Accuracy:  94.62 %\n",
      "Iteration:  590\n",
      "Accuracy:  94.69999999999999 %\n",
      "Iteration:  600\n",
      "Accuracy:  94.77 %\n",
      "Iteration:  610\n",
      "Accuracy:  94.81 %\n",
      "Iteration:  620\n",
      "Accuracy:  94.85 %\n",
      "Iteration:  630\n",
      "Accuracy:  94.89999999999999 %\n",
      "Iteration:  640\n",
      "Accuracy:  94.94 %\n",
      "Iteration:  650\n",
      "Accuracy:  95.0 %\n",
      "Iteration:  660\n",
      "Accuracy:  95.06 %\n",
      "Iteration:  670\n",
      "Accuracy:  95.1 %\n",
      "Iteration:  680\n",
      "Accuracy:  95.14 %\n",
      "Iteration:  690\n",
      "Accuracy:  95.21 %\n",
      "Iteration:  700\n",
      "Accuracy:  95.25 %\n",
      "Iteration:  710\n",
      "Accuracy:  95.32000000000001 %\n",
      "Iteration:  720\n",
      "Accuracy:  95.34 %\n",
      "Iteration:  730\n",
      "Accuracy:  95.38 %\n",
      "Iteration:  740\n",
      "Accuracy:  95.43 %\n",
      "Iteration:  750\n",
      "Accuracy:  95.48 %\n",
      "Iteration:  760\n",
      "Accuracy:  95.5 %\n",
      "Iteration:  770\n",
      "Accuracy:  95.55 %\n",
      "Iteration:  780\n",
      "Accuracy:  95.58 %\n",
      "Iteration:  790\n",
      "Accuracy:  95.61 %\n",
      "Iteration:  800\n",
      "Accuracy:  95.64 %\n",
      "Iteration:  810\n",
      "Accuracy:  95.67 %\n",
      "Iteration:  820\n",
      "Accuracy:  95.73 %\n",
      "Iteration:  830\n",
      "Accuracy:  95.77 %\n",
      "Iteration:  840\n",
      "Accuracy:  95.8 %\n",
      "Iteration:  850\n",
      "Accuracy:  95.83 %\n",
      "Iteration:  860\n",
      "Accuracy:  95.86 %\n",
      "Iteration:  870\n",
      "Accuracy:  95.89999999999999 %\n",
      "Iteration:  880\n",
      "Accuracy:  95.92 %\n",
      "Iteration:  890\n",
      "Accuracy:  95.94 %\n",
      "Iteration:  900\n",
      "Accuracy:  95.98 %\n",
      "Iteration:  910\n",
      "Accuracy:  96.00999999999999 %\n",
      "Iteration:  920\n",
      "Accuracy:  96.04 %\n",
      "Iteration:  930\n",
      "Accuracy:  96.06 %\n",
      "Iteration:  940\n",
      "Accuracy:  96.1 %\n",
      "Iteration:  950\n",
      "Accuracy:  96.12 %\n",
      "Iteration:  960\n",
      "Accuracy:  96.15 %\n",
      "Iteration:  970\n",
      "Accuracy:  96.17 %\n",
      "Iteration:  980\n",
      "Accuracy:  96.22 %\n",
      "Iteration:  990\n",
      "Accuracy:  96.24000000000001 %\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "W1, b1, W2, b2 = grad_descent(X_train, Y_train, 1000, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to output a prediction after having trained the model\n",
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Calculate preditction\n",
    "    \n",
    "    Args:\n",
    "        X (array): The features set that we want to make a prediction on\n",
    "        W1 (array): The first weights matrix\n",
    "        b1 (array): The first bias vector\n",
    "        W2 (array): The second weights matrix\n",
    "        b2 (array): The second bias vector\n",
    "        \n",
    "    Returns:\n",
    "        Outputs the prediction (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate A2 (Activation of the output layer) from the forward propagation\n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    \n",
    "    # Get prediction from A2\n",
    "    predictions = get_predictions(A2)\n",
    "    # outs = A2\n",
    "    return predictions, A2\n",
    "\n",
    "\n",
    "# Define a function to test a prediction \n",
    "def test_prediction(index, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Calculation to give a prediction and show the image\n",
    "    \n",
    "    Args:\n",
    "        index (int): Index of the input that we want to predict\n",
    "        W1 (array): The first weights matrix\n",
    "        b1 (array): The first bias vector\n",
    "        W2 (array): The second weights matrix\n",
    "        b2 (array): The second bias vector\n",
    "        \n",
    "    Returns:\n",
    "        Prediction of the digit (int)\n",
    "        Image of the digit using plt\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the image by slicing the features dataset through the index\n",
    "    current_image = X_dev_test[:, index, None]\n",
    "    \n",
    "    # Make a prediction of the input with the given index\n",
    "    prediction, A2 = make_predictions(X_dev_test[:, index, None], W1, b1, W2, b2)\n",
    "    # label = Y_dev[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(np.round(A2, 3))\n",
    "    # for i in outs:\n",
    "    #    print(round(i[0],4))\n",
    "    # print(\"Label: \", label)\n",
    "    \n",
    "    # Reshape our image from a vector fo 784 elements to a 28 x 28 grid, multiply by 255 to rescale to original\n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    \n",
    "    # Plot the image/input\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  94.19999999999999 %\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy and output of the prediction\n",
    "dev_predictions, _ = make_predictions(X_dev, W1, b1, W2, b2)\n",
    "# get_accuracy(dev_predictions, Y_dev)\n",
    "print('Accuracy: ', round(get_accuracy(dev_predictions, Y_dev), 4) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_dev_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [0]\n",
      "[[0.978]\n",
      " [0.   ]\n",
      " [0.019]\n",
      " [0.   ]\n",
      " [0.   ]\n",
      " [0.   ]\n",
      " [0.002]\n",
      " [0.   ]\n",
      " [0.   ]\n",
      " [0.   ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test any image/input that you choose\n",
    "test_prediction(random.randint(0,28000), W1, b1, W2, b2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
